{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleaning and Meta Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sbs\n",
    "\n",
    "# Options for plots\n",
    "plt.rcParams['figure.figsize'] = (10, 8)\n",
    "sbs.set('paper')\n",
    "\n",
    "# Import litstudy\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)\n",
    "import litstudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data derived from scopus and its cleaning\n",
    "df = pd.read_csv(\"scopus.csv\")\n",
    "df=df.drop_duplicates('DOI')\n",
    "df = df.drop(['Author full names','Author(s) ID','Link','Source'],  axis =1)\n",
    "df.to_csv('cleanedscopus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleanedscopus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the cleaned dataset \n",
    "\n",
    "df_filtered = df[(df['Year'] >= 2015) & (df['Year'] <= 2024)]\n",
    "\n",
    "#citation count ranges\n",
    "citation_ranges = [(0, 0), (1, 5), (6, 10), (11, 20), (21, 30), (31, 50), (51, float('inf'))]\n",
    "range_labels = ['0', '1-5', '6-10', '11-20', '21-30', '31-50', '51+']\n",
    "colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'lightblue', 'orchid', 'lightcoral']\n",
    "# Categorize papers based on their publication year and citation count ranges\n",
    "df_filtered['CitationRange'] = pd.cut(df_filtered['Cited by'], bins=[low - 0.1 for low, high in citation_ranges] + [float('inf')], labels=range_labels)\n",
    "\n",
    "#Papers by year\n",
    "grouped = df_filtered.groupby('Year')\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(12, 5))\n",
    "\n",
    "# Bar charts for each year\n",
    "for (year, data), ax in zip(grouped, axes.flatten()):\n",
    "    citation_counts = data['CitationRange'].value_counts().sort_index()\n",
    "    ax.bar(citation_counts.index, citation_counts, color=colors)\n",
    "    ax.set_title(f' {year}', fontsize=10)\n",
    "    ax.set_xlabel('Citation Range', fontsize=10)\n",
    "    ax.set_ylabel('Number of Papers', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)  # Increase tick label font size\n",
    "    ax.set_xticklabels(citation_counts.index, rotation=45)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication per year\n",
    "df_filtered = df[df['Year'] > 2015]\n",
    "\n",
    "papers_per_year = df_filtered.groupby('Year').size().reset_index(name='Paper_Count')\n",
    "\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1, len(papers_per_year)))\n",
    "\n",
    "# Plotting the number of papers published per year\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(papers_per_year['Year'], papers_per_year['Paper_Count'], color=colors)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(papers_per_year['Year'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter journal articles\n",
    "journal_df = df[df['Document Type'] == 'Article']\n",
    "\n",
    "# Count for top 10 journals\n",
    "top_journals = journal_df['Source title'].value_counts().head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "top_journals.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Top 10 Journals by Number of Papers')\n",
    "ax.set_xlabel('Journal')\n",
    "ax.set_ylabel('Number of Papers')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter  conference papers\n",
    "conf_df = df[df['Document Type'] == 'Conference paper']\n",
    "\n",
    "#Count for top 10 conferences\n",
    "top_confs = conf_df['Source title'].value_counts().head(20)\n",
    "plt.figure(figsize=(8,6))\n",
    "top_confs.plot(kind='bar')\n",
    "plt.title('Top 10 Conferences by Number of Papers')\n",
    "plt.xlabel('Conference')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting author affiliations from the dataset\n",
    "\n",
    "author_affil_list = []\n",
    "\n",
    "for entry in df[\"Authors with affiliations\"].dropna():\n",
    "    parts = [p.strip() for p in entry.split(';') if p.strip()]\n",
    "    author_affil_list.extend(parts)\n",
    "\n",
    "total_authors = len(author_affil_list)\n",
    "\n",
    "print(f\"Total number of authors: {total_authors}\\n\")\n",
    "print(\"Sample author-affiliation pairs:\")\n",
    "for i, pair in enumerate(author_affil_list[:5], 1):\n",
    "    print(f\"{i}. {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top 10 most common countries\n",
    "from collections import Counter\n",
    "\n",
    "# Function to extract country\n",
    "def extract_country(affiliation):\n",
    "    try:\n",
    "        return affiliation.split(',')[-1].strip()\n",
    "    except:\n",
    "        return None\n",
    "countries = [extract_country(aff) for aff in author_affil_list]\n",
    "countries = [c for c in countries if c]\n",
    "country_counts = Counter(countries)\n",
    "# Get top 10 most common countries\n",
    "top_10 = country_counts.most_common(10)\n",
    "print(\"Top 10 Countries by Frequency:\\n\")\n",
    "for country, count in top_10:\n",
    "    print(f\"{country}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country(affiliation):\n",
    "    try:\n",
    "        return affiliation.split(',')[-1].strip()\n",
    "    except:\n",
    "        return None\n",
    "countries = [extract_country(aff) for aff in author_affil_list]\n",
    "countries = [c for c in countries if c]   \n",
    "\n",
    "country_counts = Counter(countries)\n",
    "top_10 = country_counts.most_common(10)\n",
    "\n",
    "# Split into names and counts\n",
    "country_names = [c for c, _ in top_10]\n",
    "frequencies   = [n for _, n in top_10]\n",
    " \n",
    "colors = plt.cm.Blues(np.linspace(1, 0.5, len(frequencies)))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "plt.figure(figsize=(5, 6))\n",
    "plt.barh(country_names, frequencies, color=colors)\n",
    "plt.xlabel('Number of Authors')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Top 10 Countries by Author Affiliation Frequency')\n",
    "plt.gca().invert_yaxis()  # Highest value on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- CONFIG ----\n",
    "POSSIBLE_AFFIL_COLS = [\"Authors with affiliations\", \"Affiliations\", \"Author Affiliations\"]\n",
    "SPLIT_CHAR = ';'            # how multiple affiliations are separated in a cell\n",
    "TOP_N_DEFAULT = 15          # fallback if graph is large\n",
    "\n",
    "# ---- PICK THE AFFILIATION COLUMN SAFELY ----\n",
    "affil_col = None\n",
    "for c in POSSIBLE_AFFIL_COLS:\n",
    "    if c in df.columns:\n",
    "        affil_col = c\n",
    "        break\n",
    "if affil_col is None:\n",
    "    # fallback to first column, but warn\n",
    "    affil_col = df.columns[0]\n",
    "    print(f\"⚠️ Using first column '{affil_col}' as affiliations (expected one of {POSSIBLE_AFFIL_COLS}).\")\n",
    "\n",
    "# ---- OPTIONAL: AUTHOR–AFFIL LIST PREVIEW ----\n",
    "author_affil_list = []\n",
    "if \"Authors with affiliations\" in df.columns:\n",
    "    for entry in df[\"Authors with affiliations\"].dropna():\n",
    "        parts = [p.strip() for p in str(entry).split(SPLIT_CHAR) if p.strip()]\n",
    "        author_affil_list.extend(parts)\n",
    "    total_authors = len(author_affil_list)\n",
    "    print(f\"Total number of authors: {total_authors}\\n\")\n",
    "    print(\"Sample author-affiliation pairs:\")\n",
    "    for i, pair in enumerate(author_affil_list[:5], 1):\n",
    "        print(f\"{i}. {pair}\")\n",
    "\n",
    "# ---- COUNTRY EXTRACTION ----\n",
    "def extract_country(affiliation: str) -> str | None:\n",
    "    if not isinstance(affiliation, str) or not affiliation.strip():\n",
    "        return None\n",
    "    # naive heuristic: take the last comma-separated token\n",
    "    parts = [p.strip() for p in affiliation.split(',') if p.strip()]\n",
    "    return parts[-1] if parts else None\n",
    "\n",
    "# ---- BUILD COUNTRY–COUNTRY COLLAB NETWORK ----\n",
    "G = nx.Graph()\n",
    "\n",
    "for raw in df[affil_col].dropna():\n",
    "    affils = [a.strip() for a in str(raw).split(SPLIT_CHAR) if a.strip()]\n",
    "    countries = [extract_country(a) for a in affils]\n",
    "    countries = [c for c in countries if c]\n",
    "\n",
    "    # add weighted edges between unique country pairs per record\n",
    "    for c1, c2 in combinations(sorted(set(countries)), 2):\n",
    "        if G.has_edge(c1, c2):\n",
    "            G[c1][c2][\"weight\"] += 1\n",
    "        else:\n",
    "            G.add_edge(c1, c2, weight=1)\n",
    "\n",
    "if G.number_of_edges() == 0:\n",
    "    print(\"Not enough collaborations to draw a network (need at least one edge).\")\n",
    "else:\n",
    "    # pick top-N nodes by weighted degree\n",
    "    N = min(TOP_N_DEFAULT, G.number_of_nodes())\n",
    "    top_nodes = sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:N]\n",
    "    top_nodes = [n for n, _ in top_nodes]\n",
    "    G_top = G.subgraph(top_nodes).copy()\n",
    "\n",
    "    if G_top.number_of_edges() == 0:\n",
    "        print(f\"Top-{N} subgraph has no edges; consider increasing N or revising parsing.\")\n",
    "    else:\n",
    "        edge_weights = [d.get('weight', 1) for _, _, d in G_top.edges(data=True)]\n",
    "        width_scale = 0.25\n",
    "        widths = [max(1.0, w * width_scale) for w in edge_weights]\n",
    "\n",
    "        # layout & draw\n",
    "        plt.figure(figsize=(9, 7))\n",
    "        pos = nx.spring_layout(G_top, k=0.7, seed=42)\n",
    "\n",
    "        nx.draw_networkx_nodes(\n",
    "            G_top, pos, node_size=1200, node_color='lightblue',\n",
    "            edgecolors='gray', linewidths=1.2\n",
    "        )\n",
    "        # simple gray edges scaled by weight (keeps it readable)\n",
    "        nx.draw_networkx_edges(G_top, pos, width=widths, alpha=0.9)\n",
    "\n",
    "        for node, (x, y) in pos.items():\n",
    "            plt.text(\n",
    "                x, y, node,\n",
    "                fontsize=10, fontweight='bold',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.25')\n",
    "            )\n",
    "\n",
    "        plt.title(\"Top Countries Collaboration Network\")\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Preprocessing and Analysis with LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import Phrases, CoherenceModel\n",
    "from gensim.models import TfidfModel, LdaModel, CoherenceModel\n",
    "from gensim.utils import lemmatize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleanedscopus.csv')\n",
    "print(df.columns) \n",
    "data = df['Abstract'].dropna().values.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    return text\n",
    "\n",
    "# Define word groups for each topic\n",
    "topic_words = [\n",
    "    ['network'],\n",
    "    ['blockchain'],\n",
    "    ['information'],\n",
    "    ['systems'],\n",
    "    ['users'],\n",
    "    ['challenges'],\n",
    "    ['security'], ['privacy'],\n",
    "    ['management'],\n",
    "    ['data'],\n",
    "    ['integrity'],\n",
    "    ['protection'],\n",
    "    ['sharing'],\n",
    "    ['process'],\n",
    "    ['access']\n",
    "]\n",
    "\n",
    "# Generate the word co-occurrence matrix\n",
    "def generate_co_word_heatmap(df, topic_words):\n",
    "    df['processed_text'] = df['Abstract'].apply(preprocess)\n",
    "    vectorizer = CountVectorizer(vocabulary=list(dict.fromkeys([word for topic in topic_words for word in topic])))\n",
    "    x_counts = vectorizer.fit_transform(df['processed_text'])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    co_occurrence = (x_counts.T @ x_counts).toarray()\n",
    "    np.fill_diagonal(co_occurrence, 0)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(co_occurrence, xticklabels=terms, yticklabels=terms, cmap=\"YlGnBu\", annot=True, annot_kws={\"size\": 6})\n",
    "    plt.title(\"Co-Word Heatmap for Selected Topics\")\n",
    "    plt.show()\n",
    "\n",
    "# Generate the heatmap\n",
    "generate_co_word_heatmap(df, topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of data\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    # 1. Tokenize & remove stopwords\n",
    "    base = [\n",
    "        [w for w in simple_preprocess(str(doc), deacc=True)\n",
    "         if w not in STOPWORDS]\n",
    "        for doc in texts\n",
    "    ]\n",
    "    \n",
    "    # 2. Build bigram & trigram models\n",
    "    bigram = Phrases(base, min_count=5, threshold=100)\n",
    "    trigram = Phrases(bigram[base], threshold=100)\n",
    "    bigram_mod = Phraser(bigram)\n",
    "    trigram_mod = Phraser(trigram)\n",
    "    phrased = [trigram_mod[bigram_mod[doc]] for doc in base]\n",
    "\n",
    "    # 3. Lemmatization using gensim (default POS='n', but can specify)\n",
    "    lemmatized = []\n",
    "    for doc in phrased:\n",
    "        lemmas = [w.decode('utf-8').split('/')[0] for w in lemmatize(\" \".join(doc))]\n",
    "        lemmatized.append([w for w in lemmas if w not in STOPWORDS and len(w) > 2])\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "processed_abstracts = preprocess_texts(df['Abstract'])\n",
    "\n",
    "# Dictionary, corpus, and TF-IDF\n",
    "dictionary = Dictionary(processed_abstracts)\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in processed_abstracts]\n",
    "tfidf = TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf[corpus_bow]\n",
    "\n",
    "# Train LDA\n",
    "num_topics = 40\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus_tfidf,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=20,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Compute coherence\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=processed_abstracts, dictionary=dictionary, coherence='c_v')\n",
    "print(\"Coherence Score (c_v):\", coherence_model.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the word cloud for the topics with higher Coherence score based \n",
    "processed_abstracts = preprocess_texts(df['Abstract']) \n",
    "corpus = litstudy.build_corpus(processed_abstracts, ngram_threshold=0.45)\n",
    "num_topics = 30\n",
    "topic_model = litstudy.train_nmf_model(corpus, num_topics, max_iter=300)\n",
    "plt.figure(figsize=(60, 30))\n",
    "litstudy.plot_topic_clouds(topic_model, ncols=5)\n",
    "plt.savefig('cloud.pdf', bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the trending topics graph for merged topics from the word cloud\n",
    "\n",
    "\n",
    "df_relevant = preprocess_texts(df['Abstract'].tolist())\n",
    "\n",
    "# Selected topics based on the cohrence score topics and their associated keywords\n",
    "topics_keywords = {\n",
    "    'Topic 1': ['medical', 'emrs', 'records', 'electronic_medical', 'medical_records'],\n",
    "    'Topic 2': ['ehr', 'health', 'electronic_health', 'records', 'electronic'],\n",
    "    'Topic 3': ['bitcoin', 'education', 'transactions', 'block', 'payment'],\n",
    "    'Topic 4': ['hyperledger', 'fabric', 'hyperledger_fabric', 'permissioned', 'file'],\n",
    "    'Topic 5': ['crowdsourcing', 'reputation', 'task', 'workers', 'mobile'],\n",
    "    'Topic 6': ['detection', 'video', 'image', 'deep', 'intrusion'],\n",
    "    'Topic 7': ['edge', 'edge_computing', 'computing', 'mec', 'resource'],\n",
    "    'Topic 8': ['encryption', 'attribute', 'scheme', 'abe', 'attribute_encryption'],\n",
    "    'Topic 9': ['access_control', 'access', 'control', 'policy', 'attribute'],\n",
    "    'Topic 10': ['research', 'review', 'survey', 'future', 'applications'],\n",
    "    'Topic 11': ['voting', 'electronic_voting', 'voter', 'election', 'electronic'],\n",
    "    'Topic 12': ['supply', 'supply_chain', 'chain', 'food', 'product'],\n",
    "    'Topic 13': ['learning', 'federated', 'federated_learning', 'training', 'model'],\n",
    "    'Topic 14': ['authentication', 'cross', 'vanets', 'domain', 'cross_domain'],\n",
    "    'Topic 15': ['iot', 'devices', 'iot_devices', 'internet_things', 'things'],\n",
    "    'Topic 16': ['iiot', 'industry', 'industrial_internet', 'things_iiot', 'internet'],\n",
    "    'Topic 17': ['healthcare', 'patient', 'health', 'records', 'care'],\n",
    "    'Topic 18': ['vehicles', 'iov', 'internet_vehicles', 'vehicular', 'transportation'],\n",
    "    'Topic 19': ['energy', 'energy_trading', 'renewable', 'evs', 'renewable_energy'],\n",
    "    'Topic 20': ['grid', 'smart_grid', 'smart', 'power', 'aggregation'],\n",
    "    'Topic 21': ['location', 'knowledge', 'zero', 'zero_knowledge', 'proof'],\n",
    "    'Topic 22': ['home', 'smart_home', 'smart', 'iot', 'devices'],\n",
    "    'Topic 23': ['contracts', 'smart_contracts', 'protocol', 'smart', 'transactions'],\n",
    "    'Topic 24': ['iomt', 'medical', 'things_iomt', 'devices', 'healthcare'],\n",
    "    'Topic 25': ['cloud', 'cloud_computing', 'computing', 'fog', 'storage'],\n",
    "    'Topic 26': ['identity', 'identity_management', 'ssi', 'sovereign', 'self_sovereign'],\n",
    "    'Topic 27': ['cities', 'smart_cities', 'smart', 'infrastructure', 'transportation'],\n",
    "    'Topic 28': ['sharing', 'personal', 'information', 'management', 'trust'],\n",
    "    'Topic 29': ['metaverse', 'virtual', 'social', 'virtual_world', 'reality'],\n",
    "    'Topic 30': ['trading', 'energy_trading', 'electricity', 'peer', 'market']\n",
    "}\n",
    "\n",
    "# Carefully selected topics and define a new merged topics \n",
    "merged_topics = {\n",
    "    'Privacy of personal information in Healthcare': ['Topic 1', 'Topic 2', 'Topic 17',  'Topic 24', 'Topic 28', 'Topic 4', 'Topic 5', 'Topic 10'],\n",
    "    'Identity Management in E-education and E-voting systems': ['Topic 11'],\n",
    "    'Security Measures using IDS': ['Topic 6'],\n",
    "    'Data privacy on Cloud and Edge computing': ['Topic 7', 'Topic 25'],\n",
    "    'Access-Control and authentication management in supply chain using smart contracts': ['Topic 9','Topic 3', 'Topic 14', 'Topic 12', 'Topic 23', 'Topic 26'],\n",
    "    'Advanced encryption protocols using ZKP': ['Topic 8', 'Topic 21'],\n",
    "    'Privacy protection in Distributed Systems using FL': ['Topic 13'],\n",
    "    'Security and privacy in IoT-driven systems and smart cities': ['Topic 15', 'Topic 16', 'Topic 18', 'Topic 22', 'Topic 27'],\n",
    "    'Data privacy in Energy trading over smart grid': ['Topic 19', 'Topic 20', 'Topic 30'],\n",
    "    'Privacy of real identity in Metaverse': ['Topic 29'],\n",
    "}\n",
    "\n",
    "# Function to assign topic to a publication\n",
    "def assign_topic(title, abstract, topics_keywords):\n",
    "    combined_text = f\"{title} {abstract}\".lower()\n",
    "    topic_scores = {topic: sum(combined_text.count(keyword) for keyword in keywords) for topic, keywords in topics_keywords.items()}\n",
    "    assigned_topic = max(topic_scores, key=topic_scores.get)\n",
    "    return assigned_topic\n",
    "\n",
    "# Function to assign merged topic to a publication based on the original topic\n",
    "def assign_merged_topic(assigned_topic, merged_topics):\n",
    "    for key, value in merged_topics.items():\n",
    "        if assigned_topic in value:\n",
    "            return key\n",
    "    return assigned_topic  # Default to the assigned topic if no merge is found\n",
    "\n",
    "# Assign topics to each publication\n",
    "df_relevant['Assigned_Topic'] = df_relevant.apply(lambda row: assign_topic(row['Title'], row['Abstract'], topics_keywords), axis=1)\n",
    "\n",
    "# Assign merged topics based on the original assigned topics\n",
    "df_relevant['Merged_Topic'] = df_relevant['Assigned_Topic'].apply(lambda x: assign_merged_topic(x, merged_topics))\n",
    "\n",
    "# Filter data for years 2016 to 2024\n",
    "df_filtered = df_relevant[(df_relevant['Year'] >= 2016) & (df_relevant['Year'] <= 2024)]\n",
    "\n",
    "# Aggregate counts by year and merged topic\n",
    "yearly_topic_counts = df_filtered.groupby(['Year', 'Merged_Topic']).size().unstack(fill_value=0)\n",
    "\n",
    "# Display the aggregated data\n",
    "print(yearly_topic_counts)\n",
    "\n",
    "# Visualization: Line Graph\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for column in yearly_topic_counts.columns:\n",
    "    plt.plot(yearly_topic_counts.index, yearly_topic_counts[column], marker='o', label=column)\n",
    "\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Publications', fontsize=14)\n",
    "plt.title('Topic Trend Analysis Over the Years', fontsize=16)\n",
    "plt.legend(title='Merged Topic', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('trending.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dendrogram of Topics =====\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def topic_labels_from_lda(lda_model, dictionary, topn: int = 6) -> List[str]:\n",
    "    \"\"\"Create readable labels by joining top terms per topic.\"\"\"\n",
    "    labels = []\n",
    "    for t in range(lda_model.num_topics):\n",
    "        terms = lda_model.show_topic(t, topn=topn)  # list of (term, prob)\n",
    "        label = \" \".join([w for w, _ in terms])\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def dendrogram_from_topic_matrix(topic_term_matrix: np.ndarray,\n",
    "                                labels: List[str],\n",
    "                                out_path: str = \"dendrogram.pdf\",\n",
    "                                title: str = \"Hierarchical Clustering Dendrogram\",\n",
    "                                leaf_font_size: int = 10,\n",
    "                                figsize=(12, 16)):\n",
    "    \"\"\"\n",
    "    topic_term_matrix: shape (n_topics, vocab_size), rows are topic vectors.\n",
    "    labels: list of length n_topics with human-readable topic labels.\n",
    "    \"\"\"\n",
    "    # Normalize topic vectors so cosine distance is meaningful\n",
    "    row_norm = np.linalg.norm(topic_term_matrix, axis=1, keepdims=True)\n",
    "    row_norm[row_norm == 0] = 1.0\n",
    "    X = topic_term_matrix / row_norm\n",
    "    D = pdist(X, metric=\"cosine\")\n",
    "    Z = linkage(D, method=\"average\")\n",
    "\n",
    "    # Plot (orientation='right' for long labels)\n",
    "    plt.figure(figsize=figsize)\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        labels=labels,\n",
    "        orientation='right',\n",
    "        leaf_font_size=leaf_font_size,\n",
    "        color_threshold=None,\n",
    "        above_threshold_color='k'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Topics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "    print(f\"Saved dendrogram to {out_path}\")\n",
    "\n",
    "def labels_from_components(components: np.ndarray,\n",
    "                           id2term: List[str],\n",
    "                           topn: int = 6) -> List[str]:\n",
    "    labels = []\n",
    "    for k in range(components.shape[0]):\n",
    "        top_idx = np.argsort(components[k])[::-1][:topn]\n",
    "        words = [id2term[i] for i in top_idx]\n",
    "        labels.append(\" \".join(words))\n",
    "    return labels\n",
    "try:\n",
    "\n",
    "    id2term = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "    if 'topic_model' in globals():\n",
    "        components = topic_model.components_   \n",
    "        nmf_labels = labels_from_components(components, id2term, topn=6)\n",
    "        dendrogram_from_topic_matrix(components, nmf_labels, out_path=\"dendrogram.pdf\")\n",
    "except Exception as e:\n",
    "    print(\"Could not derive NMF labels automatically. Ensure id2term matches components columns.\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
